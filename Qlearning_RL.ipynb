{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q-Learning idea:** training an agent to navigate a pupolar enviornment form the Open AI gym."
      ],
      "metadata": {
        "id": "1eBG2-4yxCp1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nwvRDhbtYkvA"
      },
      "outputs": [],
      "source": [
        "#Import open ai gym \n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading an enviorment \n",
        "\n",
        "env = gym.make('FrozenLake-v1')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VZ0REAJYrzu",
        "outputId": "c9077e88-2335-4046-cd32-02183d1642ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space.n)  #number of states\n",
        "print(env.action_space.n) #number of actions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3CXIeb1Yr2r",
        "outputId": "eacbfe93-0a9a-4e1c-e872-b08555d62bcd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be3YSxviYr5k",
        "outputId": "10654a66-596e-4754-e28a-dbfb38cf2bab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action = env.action_space.sample()\n",
        "print(action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUi82IeFYr8x",
        "outputId": "6b633158-648b-4f98-d6d8-4fad9dae3da8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation , reward , done , info = env.step(action)\n"
      ],
      "metadata": {
        "id": "IjDW9o_XYr_p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z9Wbb6CeHX9",
        "outputId": "b161eeeb-5bff-4115-f7b5-afee0c928a6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#env.render()"
      ],
      "metadata": {
        "id": "rG3X7zDHYsDC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the Q table**"
      ],
      "metadata": {
        "id": "7aqwJ-RIfsT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym \n",
        "import numpy as np \n",
        "import time \n",
        "\n",
        "env = gym.make('FrozenLake-v1')\n",
        "States = env.observation_space.n\n",
        "Actions = env.action_space.n"
      ],
      "metadata": {
        "id": "wDs8KzXXdVx_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create matrix with 0  values \n",
        "Q = np.zeros((States, Actions))\n",
        "\n",
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZiEj1p2lItL",
        "outputId": "5c13f0b3-be4f-4918-dfb5-932333044c8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Episods = 10000\n",
        "Max_steps = 100\n",
        "Learning_rate =0.81\n",
        "Gamma = 0.96\n",
        "\n",
        "RENDER = False \n"
      ],
      "metadata": {
        "id": "jF3NBmgXlIwu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.9 #random of picking actions \n",
        "\n",
        "if np.random.uniform(0,1) < epsilon: #checking if the rnadom value is inferieur than epsilon \n",
        "  action = env.action_space.sample()  #taking a random action\n",
        "else: \n",
        "  action = np.argmax(Q[state, :]) #use the q table \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5XMe3XOulI9h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Formule of updating\n",
        "Q[state, action] = Q[state, action] +Learning_rate*(reward +Gamma*np.max(Q[state,:]) - Q[state, action] )"
      ],
      "metadata": {
        "id": "pQO9sJMwlJGP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "6a018118-4fc0-4996-a518-b6586ea25212"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c285a9b96744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Formule of updating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mLearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mGamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Rewards = []\n",
        "\n",
        "for episode in range(Episods): \n",
        "  state = env.reset()\n",
        "  for n in range(Max_steps):\n",
        "    if RENDER:\n",
        "      env.render()\n",
        "    if np.random.uniform(0,1) < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    else : \n",
        "      action = np.argmax(Q[state,:])\n",
        "\n",
        "    next_state , reward , done , _ = env.step(action)\n",
        "\n",
        "    Q[state, action] = Q[state, action] +Learning_rate*(reward +Gamma*np.max(Q[next_state,:]) - Q[state, action] )\n",
        "    \n",
        "    state = next_state \n",
        "\n",
        "    if done : \n",
        "      Rewards.append(reward)\n",
        "      epsilon -= 0.001\n",
        "      break\n",
        "    \n",
        "\n",
        "print(Q)\n",
        "print(f\"Average rewards: {sum(Rewards)/len(Rewards)} \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "VGs-rcpwzTUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde63a02-4460-4c5a-f8d4-1100ff710079"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6.76325858e-04 9.03175958e-04 3.13380120e-01 1.00427930e-03]\n",
            " [6.07749964e-04 8.35223680e-04 5.51175215e-04 1.08425975e-01]\n",
            " [7.22040630e-04 3.58387231e-04 6.67890450e-04 3.58499002e-02]\n",
            " [2.84183904e-04 4.75261847e-04 2.95659185e-04 1.05920699e-02]\n",
            " [4.61574428e-01 6.12775711e-04 2.28028779e-04 1.21416174e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.20148896e-03 1.05218819e-06 1.48888142e-06 1.26846383e-06]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.13393477e-04 2.95715227e-04 6.02288806e-04 6.62086265e-01]\n",
            " [2.34494703e-04 6.64455775e-01 2.77504119e-04 3.23865269e-04]\n",
            " [1.55619679e-01 5.41325832e-05 2.90816047e-05 9.95113415e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.74220047e-02 2.30682832e-02 8.58590197e-01 2.86627695e-02]\n",
            " [9.35926042e-02 9.88384111e-01 6.98465418e-02 9.74117602e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Average rewards: 0.5123 \n"
          ]
        }
      ]
    }
  ]
}